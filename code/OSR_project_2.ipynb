{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fatty-pendant",
   "metadata": {},
   "source": [
    "# Leren en Beslissen - Open Set Recognition 1\n",
    "\n",
    "This is the last notebook of the project and it consists of the following topic:  \n",
    "\n",
    "4. Retraining for Outlier Exposure \n",
    "\n",
    "This part is based on the paper [Deep Anomaly Detection with Outlier Exposure](https://arxiv.org/abs/1812.0460) by Hendrycks et al. (2019). Do you recognize his name? Make sure to understand the method. It is a fairly simple method, but as always, scientific papers can make it sound harder than it really is. It is beneficial to especially understand the Outlier Exposure implementation and loss (chapter 3) and the Maximum Softmax Probability part (first paragraph of 4.3). Density estimation (4.4) is not relevant for you. The Discussion (5) about _flexibility_ and _closeness_ are also very interesting - especially since they discuss a very nice intuitive view on training and testing data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-medication",
   "metadata": {},
   "source": [
    "> **Exercise 4a** How do the unknown train images influence the softmax scores? Which direction do they push the softmax score towards? Discuss the views in the paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-hayes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard \n",
    "import numpy as np \n",
    "import math \n",
    "import time \n",
    "import pickle \n",
    "\n",
    "\n",
    "# Plotting \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# Progress bar \n",
    "import tqdm \n",
    "\n",
    "# Pytorch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "# A simple MLP \n",
    "from simpleMLP import *\n",
    "from finetune_MLP import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-remove",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder where the datasets are/should be downloaded to. \n",
    "DATA_PATH = '../data'\n",
    "# Folder where to save pretrained models to. \n",
    "TRAINED_PATH = '../trained_models'\n",
    "\n",
    "# Make the trained path folder if it doesn't already exist \n",
    "os.makedirs(TRAINED_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-young",
   "metadata": {},
   "source": [
    "## Outlier Exposure \n",
    "\n",
    "For Outlier Exposure, you train on two different data sets: the known data set and an outlier data set. The outlier dataset is used to train your model to distinguish between the known data and unknown data. This outlier dataset kind of serves as a 'unknown' dataset during training. Of course it is not really unknown, after all it is available during training, but the goal of your training function is to make sure your model would label the outlier data as unknown. \n",
    "\n",
    "It is very important to note that you cannot use the _unknown_ TEST data as the outlier training set, because this would be cheating. Rather, you pick a third dataset that serves as your outlier data for training. \n",
    "\n",
    "I will show you an example where MNIST serves as the known data set; CIFAR10 serves as the outlier data set for training.\n",
    "\n",
    "First, import the datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MNIST dataset \n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "mnist_train = MNIST(root=DATA_PATH, train=True, download=True, transform=ToTensor())\n",
    "mnist_test_set = MNIST(root=DATA_PATH, train=False, download=True, transform=ToTensor())\n",
    "\n",
    "# Define the train-val split \n",
    "mnist_train_set, mnist_val_set = torch.utils.data.random_split(mnist_train, [50000, 10000])\n",
    "\n",
    "# Create dataloaders for the val, test and train sets - put them in a dictionary \n",
    "mnist_loader = {}\n",
    "mnist_loader['train'] = data.DataLoader(mnist_train_set, batch_size=1024, shuffle=True, drop_last=False)\n",
    "mnist_loader['val'] = data.DataLoader(mnist_val_set, batch_size=1024, shuffle=True, drop_last=False)\n",
    "mnist_loader['test'] = data.DataLoader(mnist_test_set, batch_size=1024, shuffle=True, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-metabolism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CIFAR10 datasets \n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# function for transforming CIFAR10 to a 1x28x28 pixel image\n",
    "cifar_transform = transforms.Compose([transforms.Grayscale(num_output_channels=1), \n",
    "                                      transforms.RandomCrop((28,28)), \n",
    "                                      transforms.ToTensor()])\n",
    "\n",
    "# load the dataset, it will be downloaded if it is not yet in your datapath\n",
    "cifar_set = CIFAR10(root=DATA_PATH, train=False, download=True, transform=cifar_transform)\n",
    "\n",
    "# create val and train loaders\n",
    "cifar_loader = {}\n",
    "cifar_train_set, cifar_val_set = torch.utils.data.random_split(cifar_set, [7000, 3000])\n",
    "cifar_loader['train'] = data.DataLoader(cifar_train_set, batch_size=1024, shuffle=True, drop_last=False)\n",
    "cifar_loader['val'] = data.DataLoader(cifar_val_set, batch_size=1024, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-ceramic",
   "metadata": {},
   "source": [
    "Load your preferred MLP model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-silence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which pretrained model you want to load \n",
    "model_name = 'MLP_1'\n",
    "model_file_path = os.path.join(TRAINED_PATH, model_name+'.tar')\n",
    "\n",
    "# load the pretrained model \n",
    "model = SimpleMLP(28*28, [128,256], 10)\n",
    "model.load_state_dict(torch.load(model_file_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-accountability",
   "metadata": {},
   "source": [
    "### Finetune MLP\n",
    "\n",
    "Finetune your MLP model with CIFAR10 as the outlier dataset. This example below trains on all CIFAR10 classes as outliers, and all MNIST classes as 'known'. It trains for 5 epochs. \n",
    "\n",
    "> **Exercise 4b** Implement the Outlier Exposure model. What should be the loss objective? Perform your own experiments.  \n",
    "\n",
    "As you can see in the code provided, I already wrote the loss objective for you. However, I did not write the validation part yet (where the train results get validated). Nonetheless, the validation part is valuable when selecting a model. It makes sure it selects the model that generalizes the best to unseen data. Without this part the function just selects the last trained MLP model and this has the risk of overfitting. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-mixer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train your model for a given \n",
    "\n",
    "finetuned_model, val_performances, train_losses = finetune_MLP_OE(mnist_loader, cifar_loader, model, epochs=10, lr=0.01, threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'calculated validation performances are {val_performances} - since it not yet uses the validation part')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-brazil",
   "metadata": {},
   "source": [
    "As you can see, the code works fine without the validation part, but it is very much recommended to implement it. This requires you to write a function that evaluates the MLP model given the validation outlier and known datasets. \n",
    "\n",
    "Some way of evaluating could be: Given a threshold, how many _good_ predictions does the model make, how many of the outlier cases does it classify as unknown and how many of the known data does it classify correctly to their class (0 to 9 for MNIST)?\n",
    "\n",
    "When you have this evaluation function, it is trivial to implement the validation part in your train function. I have written code for this already and you could look at the code for the SimpleMLP too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train your model again, now after implementing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-dominican",
   "metadata": {},
   "source": [
    "### OE Experiments \n",
    "\n",
    "Now it is time to perform experiments. Try to make comparisons on which outlier data sets have the most preferable influence on the model. How does this compare to your baseline model? How does this compare to your ODIN implementation? \n",
    "\n",
    "> **Exercise 4b** Set up your own experiments. How does the closeness of the three data sets (outlier/known train, unknown test) relate to the performance of the OE model? Define a couple of settings with different levels of closeness. Report performance on each of the settings and represent your observations in a meaningful way. Which conclusions can you draw? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform your experiments here. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2021",
   "language": "python",
   "name": "dl2021"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
