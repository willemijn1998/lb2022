{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "victorian-peeing",
   "metadata": {},
   "source": [
    "# Leren en Beslissen - Open Set Recognition 1\n",
    "\n",
    "This is the first notebook of the project - and it consists of the following topics: \n",
    "1. The softmax, getting familiar \n",
    "2. The baseline model\n",
    "\n",
    "Some good starting points for getting familiar with Open Set Recognition are the original OSR paper [5] and the Baseline paper [2]. \n",
    "\n",
    "\n",
    "First, import some standard libraries. Feel free to add/ remove anything as you please. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "alert-surrey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard \n",
    "import numpy as np \n",
    "import math \n",
    "import time \n",
    "import pickle \n",
    "\n",
    "\n",
    "# Plotting \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# Progress bar \n",
    "import tqdm \n",
    "\n",
    "# Pytorch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "# A simple MLP \n",
    "from simpleMLP import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "medical-investing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder where the datasets are/should be downloaded to. \n",
    "DATA_PATH = '../data'\n",
    "# Folder where to save pretrained models to. \n",
    "TRAINED_PATH = '../trained_models'\n",
    "\n",
    "# Make the trained path folder if it doesn't already exist \n",
    "os.makedirs(TRAINED_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-leisure",
   "metadata": {},
   "source": [
    "## 1. The softmax, getting familiar \n",
    "\n",
    "### Pytorch \n",
    "\n",
    "Pytorch is a common framework for creating Deep Neural Nets. It is created by Facebook, and currenty the fastest growing method for Neural Networks. Another popular one is Tensorflow, which was created by Google. We will be using Pytorch since it is the main one used at the UvA and other research facilities. \n",
    "\n",
    "For a tutorial on Pytorch, you can use this [UvA Tutorial](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial2/Introduction_to_PyTorch.html) made by the UvA Deep Learning team. Try to get familiar with Pytorch, we will be using this framework from now on. \n",
    "\n",
    "### Multilayer Perceptron \n",
    "\n",
    "The network we will use for our classification is a multilayer perceptron (MLP). This is a feedforward Neural Network that is fully connected and can exist of an arbitrary number of hidden layers. To read up on MLPs, you can go to this [blogpost](https://towardsdatascience.com/multilayer-perceptron-explained-with-a-real-life-example-and-python-code-sentiment-analysis-cb408ee93141) or any other blogpost/Youtube video/Deep Learning book. \n",
    "\n",
    "In the file `simpleMLP.py` I have created a simple MLP that consists of two hidden layers, two ReLU activation functions, and a Softmax layer at the end. The model takes a flattened image and returns the **log** Softmax scores. This file also  includes a function for evaluating and training the model. \n",
    "\n",
    "You can experiment with all functions. For example, making the MLP wider, deeper, using different activation functions, etc. \n",
    "\n",
    "#### MNIST \n",
    "\n",
    "To train our MLP we need a dataset. We will use the famous MNIST dataset. This is a widely used benchmark dataset that consists of handwritten digits. Pytorch has a nice standard import function. Furthermore torchvision offers _dataloaders_ that loop through data sets, these are convenient for training. With the dataloaders you can change batch and set sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "located-crown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "194b9ca15cc6454abec0ec924db1459d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b2037ec4ba43428f2fadd7a697cd65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3927192f108c49519a3c881e5752aea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fadd396c8ad4410a9dfb832fcb614cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the dataset \n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "mnist_train = MNIST(root=DATA_PATH, train=True, download=True, transform=ToTensor())\n",
    "mnist_test_set = MNIST(root=DATA_PATH, train=False, download=True, transform=ToTensor())\n",
    "\n",
    "# Define the train-val split \n",
    "mnist_train_set, mnist_val_set = torch.utils.data.random_split(mnist_train, [50000, 10000])\n",
    "\n",
    "# Create dataloaders for the val, test and train sets - put them in a dictionary \n",
    "mnist_loader = {}\n",
    "mnist_loader['train'] = data.DataLoader(mnist_train_set, batch_size=1024, shuffle=True, drop_last=False)\n",
    "mnist_loader['val'] = data.DataLoader(mnist_val_set, batch_size=1024, shuffle=True, drop_last=False)\n",
    "mnist_loader['test'] = data.DataLoader(mnist_test_set, batch_size=1024, shuffle=True, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-frank",
   "metadata": {},
   "source": [
    "#### Training\n",
    "Now that we have our MLP and dataloader ready, we can start training our network. Again, you can change around with number of epochs, learning rate (lr), number of hidden dimensions, loss module, etc. One thing to pay attention to when training are the in_dimension and the out_dimension. For MNIST, which is an image of 1x28x28 pixels, we have input dimension (after flattening) of $1 \\times 28 \\times 28 = 784$ pixels. It has 10 classes (0 to 9) so the output dimension should be 10. When training, try to loop through different seeds to get the mean and variance. \n",
    "\n",
    "Save the model weights so that you do not need to train again when, you can just load and use it. This saves a lot of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fiscal-dominant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1] Training accuracy: 19.53%, Validation accuracy: 19.48%\n",
      "Test accuracy 19.37%\n"
     ]
    }
   ],
   "source": [
    "# Train model on MNIST for one seed \n",
    "\n",
    "best_model, val_accuracies, train_accuracies, test_acc, train_losses = train_MLP(data_loader=mnist_loader, \n",
    "                                                                   in_dimension=28*28, \n",
    "                                                                   hidden_dims=[128,256], \n",
    "                                                                   out_dimension=10, epochs=1, \n",
    "                                                                   lr=0.01, seed=0)\n",
    "\n",
    "# Save the model\n",
    "model_name = 'MLP_1'\n",
    "model_file_path = os.path.join(TRAINED_PATH, model_name+'.tar')\n",
    "torch.save(best_model.state_dict(), model_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "advised-directory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleMLP(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=10, bias=True)\n",
       "    (5): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model \n",
    "model = SimpleMLP(28*28, [128,256], 10)\n",
    "model.load_state_dict(torch.load(model_file_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-sunday",
   "metadata": {},
   "source": [
    "### Open Set Recognition \n",
    "\n",
    "For Open Set Recognition you have to take a dataset as _known_ and one as _unknown_, the _known_ dataset is the one you trained your model on, and the _unknown_ dataset should never be seen during training. \n",
    "\n",
    "We will first take the MNIST as the _known_ examples, and CIFAR10 as the _unknown_ dataset. Luckily we have already created and trained the MLP on MNIST, so we just need to define the CIFAR10 dataloader. When evaluating a model, it is important to define the \"openness\" of your problem. This means, how many _unknown_ samples will be seen during inference versus how many _known_ samples will be seen. When you start comparing certain datasets, models or OSR methods, make sure to keep the openness constant for best comparison. \n",
    "\n",
    "The CIFAR10 dataset consists of 3x32x32 pixel images, where the 3 stands for the three color channels (RGB). In order to plug this dataset into the MLP that you trained on MNIST, you have to transform the images. Pytorch has some built-in functions for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "imported-prediction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dedc514422b46d7bd8aa5f1d9715a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/cifar-10-python.tar.gz to ../data\n"
     ]
    }
   ],
   "source": [
    "# Import CIFAR10 datasets \n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# function for transforming CIFAR10 to a 1x28x28 pixel image\n",
    "cifar_transform = transforms.Compose([transforms.Grayscale(num_output_channels=1), \n",
    "                                      transforms.RandomCrop((28,28)), \n",
    "                                      transforms.ToTensor()])\n",
    "\n",
    "# load the dataset, it will be downloaded if it is not yet in your datapath\n",
    "cifar_set = CIFAR10(root=DATA_PATH, train=False, download=True, transform=cifar_transform)\n",
    "\n",
    "# subset the cifar dataset such that you will get your desired openness\n",
    "cifar_subset, _ = torch.utils.data.random_split(cifar_set, [5000, 5000])\n",
    "cifar_loader = data.DataLoader(cifar_subset, batch_size=1024, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-newfoundland",
   "metadata": {},
   "source": [
    "A visualisation of the cropped and converted to grayscale CIFAR10 images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "residential-appreciation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for x,y in cifar_loader: \n",
    "    print(x.shape)\n",
    "    plt.imshow(x[0].view(28,28), cmap='gray')\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-burns",
   "metadata": {},
   "source": [
    "#### Softmax scores \n",
    "\n",
    "Now that we have our MLP model and both datasets ready, we are going to analyse different Softmax scores. Note that the model outputs are **log** Softmaxes. \n",
    "\n",
    "> **Exercise 1A** For a model trained on certain classes, what are the differences between the softmax scores of new images of the seen classes and images of unseen classes during inference? How would you measure this? How would you collect your observations into a meaningful overview/representation? Motivate your answer. \n",
    "\n",
    "Look at the papers mentioned above for some intuition regarding the softmax scores of _known_ and _unknown_ datasets. Especially the papers [2] and [4] offer some useful intuitions regarding Softmax scores. I would recommend writing (several) functions that compare the softmax scores of the MLP model for known and unknown examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "duplicate-adult",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this block for your functions and/or visualisations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-printing",
   "metadata": {},
   "source": [
    "#### Open set splits \n",
    "\n",
    "We have now used the open-set split MNIST (as the known set) and CIFAR10 (as the unknown set). \n",
    "\n",
    "> **Exercise 1B** Which types of open-set splits cause a larger difference in softmax scores? Think about which unseen classes would differ, intuitively, more/less from the seen classes. Find your own data sets online and perform tests. Explain the differences you find. \n",
    "\n",
    "Pytorch offers some nice datasets, would be interesting to take a look at that. Some benchmark datasets that are often used for Open Set Recognition are: SVHN, TinyImageNet, CIFAR10, CIFAR100, TinyImageNet. Alternatively, you could add noise to some of the datasets to turn them into _unknown_ during inference. \n",
    "\n",
    "When comparing open-set splits, try to keep the hyperparameters/ variables as close to each other as possible. Also make sure the input dimensions of the _known_ and _unknown_ datasets are the same. Probably you have to perform transformations like for the CIFAR10 set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "broken-baptist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import/download datasets here\n",
    "\n",
    "# It is smart to create a list of splits so that you can loop through\n",
    "# these for the next exercises [(known, unknown), (...,...), ...]\n",
    "\n",
    "list_of_splits = [(mnist_loader['test'], cifar_loader), ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-insert",
   "metadata": {},
   "source": [
    "## 2. The baseline \n",
    "\n",
    "The baseline model by Hendrycks et al. (2017) uses the pure softmax scores to classify examples as known or unknown - and classify them to a class whenever it is known. \n",
    "\n",
    "> **Exercise 2A** Which metrics are suitable for reporting the performance of an open-set recognition model? Name at least 3 and explain their components. Choose your preferred metric(s) for reporting performance throughout the project.\n",
    "\n",
    "This is a theoretical exercise, so do some research on OSR metrics. Take into account that the Baseline OSR is a binary classification first, and a multiclass classification second. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-affiliate",
   "metadata": {},
   "source": [
    "<!-- Your answer:  -->\n",
    "\n",
    "... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-antenna",
   "metadata": {},
   "source": [
    "Now build the baseline method. You can use the models for the datasets as previously trained. You do not need to retrain the model, you only have to find the optimal threshold for best known-unknown classification. We keep it simple so you do not need to perform the known-set classification whenever the model returns the label \"known\". Write functions for the performance metrics found in exercise 2A. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-lightning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the baseline model here (or import from your py file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-oracle",
   "metadata": {},
   "source": [
    "> **Exercise 2B** Which threshold achieves the best performance in the baseline model? Which tradeoffs in performance can you find when increasing/decreasing the threshold? Hint: You can find\n",
    "the ideal threshold by using the validation set. \n",
    "\n",
    "Think of the \"openness\" of the open-set splits, how many unknown samples will you use per amount of known samples? Make sure this ratio is consistent throughout the project, or mention them explicitly if you want to deviate from it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "french-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-fault",
   "metadata": {},
   "source": [
    "> **Exercise 2C** Does the ideal threshold change across different data splits? If so, how? You can use the same data splits as for Question 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "square-enough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find thresholds for a few open-set splits\n",
    "\n",
    "for split in list_of_splits: \n",
    "    threshold = ... \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-postcard",
   "metadata": {},
   "source": [
    "\n",
    "Using (one or several of) the performance metrics found in 2A and the baseline model built, report performance(s) for different data splits. \n",
    "\n",
    "> **Exercise 2D** Which open set images are harder to detect than others? Explain the open-set splits you use. Report performance per open-set split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-republic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here. \n",
    "\n",
    "for split in list_of_splits: \n",
    "    baseline_performance = ... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-emperor",
   "metadata": {},
   "source": [
    "### References \n",
    "\n",
    "[1] Guo, Chuan, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. “On Calibration of Modern Neural Networks” (2017).\n",
    "\n",
    "[2] Hendrycks, Dan, and Kevin Gimpel. “A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks” (2016).\n",
    "\n",
    "[3] Hendrycks, Dan, Mantas Mazeika, and Thomas Dietterich. “Deep Anomaly Detection with Outlier Exposure” (2018).\n",
    "\n",
    "[4] Liang, Shiyu, Yixuan Li, and R Srikant. “Enhancing The Reliability of Out-of-Distribution Image Detection in Neural Networks” (2017).\n",
    "\n",
    "[5] Scheirer, W. J, A de Rezende Rocha, A Sapkota, and T. E Boult. “Toward Open Set Recognition.” IEEE transactions on pattern analysis and machine intelligence 35, no. 7 (2013): 1757–1772.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-conducting",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2021",
   "language": "python",
   "name": "dl2021"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
